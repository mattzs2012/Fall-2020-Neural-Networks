{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import glob\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import progressbar\n",
    "import random\n",
    "\n",
    "# sklearn functions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# keras functions\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create a class for a mini version of VGGNet (Simonyan and Zisserman, 2015)\n",
    "class MiniVGGNet:\n",
    "    def build(height, width, depth, classes):\n",
    "        # create the model and name it MiniVGGNet\n",
    "        model = Sequential(name = 'MiniVGGNet')\n",
    "                \n",
    "        # convolutional layer with 32 3x3 feature maps\n",
    "        model.add(Conv2D(32, (3, 3), padding = 'same', input_shape = (height, width, depth)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # convolutional layer with 32 3x3 feature maps\n",
    "        model.add(Conv2D(32, (3, 3), padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # 2x2 max pooling layer with stride 2x2\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # convolutional layer with 64 3x3 feature maps\n",
    "        model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # convolutional layer with 64 3x3 feature maps\n",
    "        model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # 2x2 max pooling layer with stride 2x2\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # flatten the activations from a square to a vector\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # fully-connected layer\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        # fully-connected layer with softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        # return the model\n",
    "        return model\n",
    "    \n",
    "class MiniGoogLeNet:\n",
    "    def convolution_module(x, K, kX, kY, stride, channelsDim, padding=\"same\"):\n",
    "        # create a CONV -> BN -> RELU sequence\n",
    "        x = Conv2D(K, (kX, kY), strides = stride, padding = padding)(x)\n",
    "        x = BatchNormalization(axis = channelsDim)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        \n",
    "        # return the output\n",
    "        return x\n",
    "    \n",
    "    def inception_module(x, numberOf1x1Kernels, numberOf3x3Kernels, channelsDim):\n",
    "        # define two \"parallel\" convolutions of size 1x1 and 3x3 concatenated across the channels dimension\n",
    "        convolution_1x1 = MiniGoogLeNet.convolution_module(x, numberOf1x1Kernels, 1, 1, (1, 1), channelsDim)\n",
    "        convolution_3x3 = MiniGoogLeNet.convolution_module(x, numberOf3x3Kernels, 3, 3, (1, 1), channelsDim)\n",
    "        x = concatenate([convolution_1x1, convolution_3x3], axis = channelsDim)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def downsample_module(x, K, channelsDim):\n",
    "        # define a CONV and POOL and then concatenate across the channels dimension\n",
    "        convolution_3x3 = MiniGoogLeNet.convolution_module(x, K, 3, 3, (2, 2), channelsDim, padding = 'valid')\n",
    "        pool = MaxPooling2D((3, 3), strides = (2, 2))(x)\n",
    "        x = concatenate([convolution_3x3, pool], axis = channelsDim)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def build(width, height, depth, classes):\n",
    "        inputShape = (height, width, depth)\n",
    "        channelsDim = -1\n",
    "        \n",
    "        if backend.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            channelsDim = 1\n",
    "        \n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape = inputShape)\n",
    "        x = MiniGoogLeNet.convolution_module(inputs, 96, 3, 3, (1, 1), channelsDim)\n",
    "        \n",
    "        # two inception modules followed by a downsample module\n",
    "        x = MiniGoogLeNet.inception_module(x, 32, 32, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 32, 48, channelsDim)\n",
    "        x = MiniGoogLeNet.downsample_module(x, 80, channelsDim)\n",
    "        \n",
    "        # four inception modules followed by a downsample module\n",
    "        x = MiniGoogLeNet.inception_module(x, 112, 48, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 96, 64, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 80, 80, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 48, 96, channelsDim)\n",
    "        x = MiniGoogLeNet.downsample_module(x, 96, channelsDim)\n",
    "        \n",
    "        # two inception modules followed by global POOL and dropout\n",
    "        x = MiniGoogLeNet.inception_module(x, 176, 160, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 176, 160, channelsDim)\n",
    "        x = AveragePooling2D((7, 7))(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        \n",
    "        # softmax classifier\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes)(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        \n",
    "        # create a model\n",
    "        model = Model(inputs, x, name='MiniGoogLeNet')\n",
    "        \n",
    "        # return the model\n",
    "        return model\n",
    "    \n",
    "class ResNet:\n",
    "    def residual_module(data, K, stride, channelsDim, reduce = False, reg = 0.0001, bnEpsilon = 0.00002, bnMomentum = 0.9):\n",
    "        shortcut = data\n",
    "        \n",
    "        # 1x1 CONVs\n",
    "        bn1 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(data)\n",
    "        act1 = Activation('relu')(bn1)\n",
    "        conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = l2(reg))(act1)\n",
    "        \n",
    "        # 3x3 CONVs\n",
    "        bn2 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(conv1)\n",
    "        act2 = Activation('relu')(bn2)\n",
    "        conv2 = Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = 'same', use_bias = False, kernel_regularizer = l2(reg))(act2)\n",
    "        \n",
    "        # 1x1 CONVs\n",
    "        bn3 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(conv2)\n",
    "        act3 = Activation('relu')(bn3)\n",
    "        conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = l2(reg))(act3)\n",
    "        \n",
    "        # if we reduce the spatial size, apply a CONV layer to the shortcut\n",
    "        if reduce:\n",
    "            shortcut = Conv2D(K, (1, 1), strides = stride, use_bias = False, kernel_regularizer = l2(reg))(act1)\n",
    "            \n",
    "        # add the shortcut and the final CONV\n",
    "        x = add([conv3, shortcut])\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def build(width, height, depth, classes, stages, filters, reg = 0.0001, bnEpsilon = 0.00002, bnMomentum = 0.9, dataset='cifar'):\n",
    "        inputShape = (height, width, depth)\n",
    "        channelsDim = -1\n",
    "        \n",
    "        if backend.image_data_format() == 'channels_first':\n",
    "            inputShape = (depth, height, width)\n",
    "            channelsDim = 1\n",
    "            \n",
    "        # set the input and apply BN\n",
    "        inputs = Input(shape = inputShape)\n",
    "        x = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(inputs)\n",
    "        \n",
    "        if dataset == 'cifar':\n",
    "            # apply a single CONV layer\n",
    "            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = 'same',\n",
    "                       kernel_regularizer = l2(reg))(x)\n",
    "        \n",
    "        # loop over the number of stages\n",
    "        for counter in range(0, len(stages)):\n",
    "            # initialize the stride\n",
    "            if counter == 0:\n",
    "                stride = (1, 1)\n",
    "            else:\n",
    "                stride = (2, 2)\n",
    "                    \n",
    "            # apply a residual module to reduce the spatial dimension of the image volume\n",
    "            x = ResNet.residual_module(x, filters[counter + 1], stride, channelsDim, reduce = True, bnEpsilon = bnEpsilon, bnMomentum = bnMomentum)\n",
    "            \n",
    "            # loop over the number of layers in the current stage\n",
    "            for j in range(0, stages[counter] - 1):\n",
    "                # apply a residual module\n",
    "                x = ResNet.residual_module(x, filters[counter + 1], (1, 1), channelsDim, bnEpsilon = bnEpsilon, bnMomentum = bnMomentum)\n",
    "                    \n",
    "        # apply BN -> ACT -> POOL\n",
    "        x = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = AveragePooling2D((8, 8))(x)\n",
    "        \n",
    "        # softmax classifier\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes, kernel_regularizer = l2(reg))(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        \n",
    "        # create the model\n",
    "        model = Model(inputs, x, name = 'ResNet')\n",
    "        \n",
    "        # return the model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5\n",
    "\n",
    "If we want to work with huge pre-trained neural nets like VGG19 or other deep CNNs, storing them takes far more space than our RAM is likely to support, so we need to store them on HDD/SDDs in an efficient way. Keras's model format is pretty large, but HDF5 is a good data format for this, but we need some code to be able to interface with this format, which we write below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5DataWriter:\n",
    "    def __init__(self, dims, outputPath, dataKey = 'images', bufferSize = 1000):\n",
    "        # check if outputpath exists\n",
    "        if os.path.exists(outputPath):\n",
    "            raise ValueError('The supplied `outputPath` already exists and cannot be overwritten. Delete '\n",
    "                            ' the file manually before continuing.', outputPath)\n",
    "            \n",
    "        # open the HDF5 database for writing and create two datasets: one to store the images/features and\n",
    "        # one to store the labels\n",
    "        self.db = h5py.File(outputPath, 'w')\n",
    "        self.data = self.db.create_dataset(dataKey, dim, dtype = 'float')\n",
    "        self.labels = self.db.create_dataset('labels', (dims[0],), dtype = 'float')\n",
    "        \n",
    "        # store the buffer size and initialize the buffer and index\n",
    "        self.bufferSize = bufferSize\n",
    "        self.buffer = {'data': [], 'labels': []}\n",
    "        self.index = 0\n",
    "        \n",
    "    def add(self, rows, labels):\n",
    "        # add the rows and labels to the buffer\n",
    "        self.buffer['data'].extend(rows)\n",
    "        self.buffer['labels'].extend(labels)\n",
    "        \n",
    "        # check if the buffer needs to be flushed to disk\n",
    "        if len(self.buffer['data']) >= self.bufferSize:\n",
    "            self.flush()\n",
    "            \n",
    "    def flush(self):\n",
    "        # write the buffer to disk and reset buffer\n",
    "        i = self.index + len(self.buffer['data'])\n",
    "        self.data[self.index:i] = self.buffer['data']\n",
    "        self.labels[self.index:i] = self.buffer['labels']\n",
    "        \n",
    "        self.index = i\n",
    "        self.buffer = {'data': [], 'labels': []}\n",
    "        \n",
    "    def storeClassLabels(self, classLabels):\n",
    "        # create a dataset to store class label names, then store them\n",
    "        dt = h5py.special_dtype(vlen = unicode)\n",
    "        labelSet = self.db.create_dataset('label_names', (len(classLabels),), dtype = dt)\n",
    "        labelSet[:] = classLabels\n",
    "        \n",
    "    def close(self):\n",
    "        # flush entries to disk if needed\n",
    "        if len(self.buffer['data']) > 0:\n",
    "            self.flush()\n",
    "            \n",
    "        # close the dataset\n",
    "        self.db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Let's write some code to extract features from an arbitrary image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(batchSize, dataset, output, bufferSize):\n",
    "\n",
    "    print('Loading images...')\n",
    "    imagePaths = list(paths.list_images(dataset))\n",
    "    random.shuffle(imagePaths)\n",
    "\n",
    "    # extract class labels from the image paths and encode the labels (assumes the\n",
    "    # file paths are in the form 'dataset_name/{class_label}/example.jpg')\n",
    "    labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(labels)\n",
    "\n",
    "\n",
    "    # load the pretrained VGG16 net on the imagenet dataset but without the final\n",
    "    # fully connected layers (include_top is False), so we get features after\n",
    "    # propagating images through the last pooling later\n",
    "    print('Loading network...')\n",
    "    model = VGG16(weights = 'imagenet', include_top = False)\n",
    "\n",
    "    # initialize the HDF5 dataset writer and store class label names in a dataset\n",
    "    dataset = HDF5DatasetWriter((len(imagePaths), 512 * 7 * 7), output,\n",
    "                                dataKey = 'features', bufferSize = 1000)\n",
    "\n",
    "    dataset.storeClassLabels(le.classes_)\n",
    "\n",
    "    # initialize the progress bar\n",
    "    widgets = ['Extracting features: ', progressbar.Percentage(), ' ', progressbar.Bar(),\n",
    "              ' ', progressbar.ETA()]\n",
    "\n",
    "    pbar = progressbar.ProgressBar(maxval = len(imagePaths), widgets = widgets).start()\n",
    "\n",
    "    # loop over the images in batches\n",
    "    for i in np.arange(0, len(imagePaths), bs):\n",
    "        # extract the batch of images/labels and initialize the list of images that will\n",
    "        # be passed through the net for feature extration\n",
    "        batchPaths = imagePaths[i:i + bs]\n",
    "        batchLabels = labels[i:i + bs]\n",
    "        batchImages = []\n",
    "\n",
    "        # loop over the images/labels in the current batch\n",
    "        for (j, imagePath) in enumerate(batchPaths):\n",
    "            # load the input image and resize it to 224x224 pixels\n",
    "            image = load_img(imagePath, target_size = (224, 224))\n",
    "            image = img_to_array(image)\n",
    "\n",
    "            # preprocess by expanding dimensions and subtracting mean RGB pixel\n",
    "            # intensity from ImageNet\n",
    "            image = np.expand_dims(image, axis = 0)\n",
    "            image = imagenet_utils.preprocess_input(image)\n",
    "\n",
    "            # add the images to the batch\n",
    "            batchImages.append(image)\n",
    "\n",
    "        # pass images through net and use outputs as features\n",
    "        batchImages = np.vstack(batchImages)\n",
    "        features = model.predict(batchImages, batch_size = batchSize)\n",
    "\n",
    "        # flattened each image to a feature vector of the MaxPooling2D outputs\n",
    "        features = features.reshape((features.shape[0], 512 * 7 * 7))\n",
    "\n",
    "        # add the featuers and labels to the HDF5 dataset\n",
    "        dataset.add(features, batchLabels)\n",
    "        pbar.update(i)\n",
    "\n",
    "        # close the dataset\n",
    "        dataset.close()\n",
    "        pbar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning By Fine-Tuning\n",
    "\n",
    "To operate on individual layers, we need to determine how to access them with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printLayerNames(include_top = True):\n",
    "    # load VGG16 pre-trained on ImageNet\n",
    "    print('Loading network...')\n",
    "    model = VGG16(weights = 'imagenet', include_top = include_top)\n",
    "    \n",
    "    # loop over the layers and display them\n",
    "    for (i, layer) in enumerate(model.layers):\n",
    "        print('{}\\t{}'.format(i, layer.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer names with the head\n",
      "Loading network...\n",
      "0\tInputLayer\n",
      "1\tConv2D\n",
      "2\tConv2D\n",
      "3\tMaxPooling2D\n",
      "4\tConv2D\n",
      "5\tConv2D\n",
      "6\tMaxPooling2D\n",
      "7\tConv2D\n",
      "8\tConv2D\n",
      "9\tConv2D\n",
      "10\tMaxPooling2D\n",
      "11\tConv2D\n",
      "12\tConv2D\n",
      "13\tConv2D\n",
      "14\tMaxPooling2D\n",
      "15\tConv2D\n",
      "16\tConv2D\n",
      "17\tConv2D\n",
      "18\tMaxPooling2D\n",
      "19\tFlatten\n",
      "20\tDense\n",
      "21\tDense\n",
      "22\tDense\n",
      "\n",
      "Layer names without the head\n",
      "Loading network...\n",
      "0\tInputLayer\n",
      "1\tConv2D\n",
      "2\tConv2D\n",
      "3\tMaxPooling2D\n",
      "4\tConv2D\n",
      "5\tConv2D\n",
      "6\tMaxPooling2D\n",
      "7\tConv2D\n",
      "8\tConv2D\n",
      "9\tConv2D\n",
      "10\tMaxPooling2D\n",
      "11\tConv2D\n",
      "12\tConv2D\n",
      "13\tConv2D\n",
      "14\tMaxPooling2D\n",
      "15\tConv2D\n",
      "16\tConv2D\n",
      "17\tConv2D\n",
      "18\tMaxPooling2D\n"
     ]
    }
   ],
   "source": [
    "print('Layer names with the head')\n",
    "printLayerNames()\n",
    "\n",
    "print('\\nLayer names without the head')\n",
    "printLayerNames(include_top = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the first 18 layers are the same, but the full network, including the top of the net, i.e. the fully connected layers. This \"top\" is also known as the \"head\".\n",
    "\n",
    "### Network Surgery\n",
    "\n",
    "Once we've chopped off the head of the net, we need to replace it with a newly initialized head so we can train it and attempt to transfer the knowledge of the lower layers of the net to a new dataset by retraining this new head.\n",
    "\n",
    "Let's create a net to go at the head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCHeadNet:\n",
    "    def build(baseModel, classes, D):\n",
    "        # initialize the head model and add a fully-connected layer\n",
    "        headModel = baseModel.output\n",
    "        headModel = Flatten(name = 'flatten')(headModel)\n",
    "        headModel = Dense(D, activation = 'relu')(headModel)\n",
    "        headModel = Dropout(0.5)(headModel)\n",
    "        \n",
    "        # add a softmax layer\n",
    "        headModel = Dense(classes, activation = 'softmax')(headModel)\n",
    "        \n",
    "        # return the model\n",
    "        return headModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Preprocessing Code\n",
    "\n",
    "This code comes from Adrian Rosebrock's *Deep Learning for Computer Vision* book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader that applies specified preprocessors\n",
    "class SimpleDatasetLoader:\n",
    "\tdef __init__(self, preprocessors=None):\n",
    "\t\t# store the image preprocessor\n",
    "\t\tself.preprocessors = preprocessors\n",
    "\n",
    "\t\t# if the preprocessors are None, initialize them as an\n",
    "\t\t# empty list\n",
    "\t\tif self.preprocessors is None:\n",
    "\t\t\tself.preprocessors = []\n",
    "\n",
    "\tdef load(self, imagePaths, verbose=-1):\n",
    "\t\t# initialize the list of features and labels\n",
    "\t\tdata = []\n",
    "\t\tlabels = []\n",
    "\n",
    "\t\t# loop over the input images\n",
    "\t\tfor (i, imagePath) in enumerate(imagePaths):\n",
    "\t\t\t# load the image and extract the class label assuming\n",
    "\t\t\t# that our path has the following format:\n",
    "\t\t\t# /path/to/dataset/{class}/{image}.jpg\n",
    "\t\t\timage = cv2.imread(imagePath)\n",
    "\t\t\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "\t\t\t# check to see if our preprocessors are not None\n",
    "\t\t\tif self.preprocessors is not None:\n",
    "\t\t\t\t# loop over the preprocessors and apply each to\n",
    "\t\t\t\t# the image\n",
    "\t\t\t\tfor p in self.preprocessors:\n",
    "\t\t\t\t\timage = p.preprocess(image)\n",
    "\n",
    "\t\t\t# treat our processed image as a \"feature vector\"\n",
    "\t\t\t# by updating the data list followed by the labels\n",
    "\t\t\tdata.append(image)\n",
    "\t\t\tlabels.append(label)\n",
    "\n",
    "\t\t\t# show an update every `verbose` images\n",
    "\t\t\tif verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
    "\t\t\t\tprint(\"[INFO] processed {}/{}\".format(i + 1,\n",
    "\t\t\t\t\tlen(imagePaths)))\n",
    "\n",
    "\t\t# return a tuple of the data and labels\n",
    "\t\treturn (np.array(data), np.array(labels))\n",
    "\n",
    "# resize images while maintaining aspect ratio\n",
    "class AspectAwarePreprocessor:\n",
    "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
    "\t\t# store the target image width, height, and interpolation\n",
    "\t\t# method used when resizing\n",
    "\t\tself.width = width\n",
    "\t\tself.height = height\n",
    "\t\tself.inter = inter\n",
    "\n",
    "\tdef preprocess(self, image):\n",
    "\t\t# grab the dimensions of the image and then initialize\n",
    "\t\t# the deltas to use when cropping\n",
    "\t\t(h, w) = image.shape[:2]\n",
    "\t\tdW = 0\n",
    "\t\tdH = 0\n",
    "\n",
    "\t\t# if the width is smaller than the height, then resize\n",
    "\t\t# along the width (i.e., the smaller dimension) and then\n",
    "\t\t# update the deltas to crop the height to the desired\n",
    "\t\t# dimension\n",
    "\t\tif w < h:\n",
    "\t\t\timage = imutils.resize(image, width=self.width,\n",
    "\t\t\t\tinter=self.inter)\n",
    "\t\t\tdH = int((image.shape[0] - self.height) / 2.0)\n",
    "\n",
    "\t\t# otherwise, the height is smaller than the width so\n",
    "\t\t# resize along the height and then update the deltas\n",
    "\t\t# crop along the width\n",
    "\t\telse:\n",
    "\t\t\timage = imutils.resize(image, height=self.height,\n",
    "\t\t\t\tinter=self.inter)\n",
    "\t\t\tdW = int((image.shape[1] - self.width) / 2.0)\n",
    "\n",
    "\t\t# now that our images have been resized, we need to\n",
    "\t\t# re-grab the width and height, followed by performing\n",
    "\t\t# the crop\n",
    "\t\t(h, w) = image.shape[:2]\n",
    "\t\timage = image[dH:h - dH, dW:w - dW]\n",
    "\n",
    "\t\t# finally, resize the image to the provided spatial\n",
    "\t\t# dimensions to ensure our output image is always a fixed\n",
    "\t\t# size\n",
    "\t\treturn cv2.resize(image, (self.width, self.height),\n",
    "\t\t\tinterpolation=self.inter)\n",
    "\n",
    "# convert images to arrays\n",
    "class ImageToArrayPreprocessor:\n",
    "\tdef __init__(self, dataFormat=None):\n",
    "\t\t# store the image data format\n",
    "\t\tself.dataFormat = dataFormat\n",
    "\n",
    "\tdef preprocess(self, image):\n",
    "\t\t# apply the Keras utility function that correctly rearranges\n",
    "\t\t# the dimensions of the image\n",
    "\t\treturn img_to_array(image, data_format=self.dataFormat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "\n",
    "Now, let's try to implement some fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fineTune(datasetPath, modelPath):\n",
    "    # initialize an image generator for data augmentation\n",
    "    aug = ImageDataGenerator(rotation_range = 30, width_shift_range = 0.1, height_shift_range = 0.1,\n",
    "                             shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True,\n",
    "                             fill_mode = 'nearest')\n",
    "    \n",
    "    # get the list of image paths and extract class labels\n",
    "    imagePaths = list(paths.list_images(datasetPath))\n",
    "    classNames = [pt.split(os.path.sep)[-2] for pt in imagePaths]\n",
    "    classNames = [str(x) for x in np.unique(classNames)]\n",
    "    \n",
    "    # initialize image preprocessors\n",
    "    aap = AspectAwarePreprocessor(224, 224)\n",
    "    iap = ImageToArrayPreprocessor()\n",
    "    \n",
    "    # load dataset and scale pixel intensities to [0, 1]\n",
    "    sdl = SimpleDatasetLoader(preprocessors = [aap, iap])\n",
    "    (data, labels) = sdl.load(imagePaths, verbose = 500)\n",
    "    data = data.astype('float') / 255.0\n",
    "    \n",
    "    # partition into training/test splits\n",
    "    (trainX, testX, trainY, testY) = train_test_split(data, labels, test_size = 0.25)\n",
    "    \n",
    "    # load VGG16 net without the head\n",
    "    baseModel = VGG16(weights = 'imagenet', include_top = False,\n",
    "                     input_tensor = Input(224, 224, 3))\n",
    "    \n",
    "    # initialize the new head\n",
    "    headModel = FCHeadNet.build(baseModel, len(classNames), 256)\n",
    "    \n",
    "    # build the model with the new head\n",
    "    model = Model(inputs = baseModel.input, outputs = headModel)\n",
    "    \n",
    "    # loop over the layers of the base model and \"freeze\" them so they are not\n",
    "    # updated during training\n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # compile the model with a small learning rate\n",
    "    print('Compiling model...')\n",
    "    opt = RMSprop(lr = 0.001)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    \n",
    "    # train the head of the network for a few iterations to initialize it a little better\n",
    "    # than randomly\n",
    "    print('Training head...')\n",
    "    model.fit(aug.flow(trainX, trainY, batch_size = 32), validation_data = (testX, testY),\n",
    "             epochs = 25, steps_per_epoch = len(trainX) // 32, verbose = 1)\n",
    "    \n",
    "    # evaluate the net after this \"smart\" initialization\n",
    "    print('Evaluating after initialization...')\n",
    "    predictions = model.predict(testX, batch_size = 32)\n",
    "    print(classification_report(testY.argmax(axis = 1), predictions.argmax(axis = 1),\n",
    "                               target_names = classNames))\n",
    "    \n",
    "    # unfreeze the final set of convolutional layers\n",
    "    for layer in baseModel.layers[15:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    # recompile the model\n",
    "    print('Re-compiling model...')\n",
    "    opt = SGD(lr = 0.001)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    \n",
    "    # train the model again, fine-tuning the last few convolutional layers plus the head\n",
    "    model.fit(aug.flow(trainX, trainY, batch_size = 32), validation_data = (testX, testY),\n",
    "             epochs = 100, steps_per_epoch = len(trainX) // 32, verbose = 1)\n",
    "    \n",
    "    # evaluate the net\n",
    "    print('Evaluating after fine-tuning...')\n",
    "    predictions = model.predict(testX, batch_size = 32)\n",
    "    print(classification_report(testY.argmax(axis = 1), predictions.argmax(axis = 1),\n",
    "                               target_names = classNames))\n",
    "    \n",
    "    # save the model\n",
    "    print('Saving the model to disk...')\n",
    "    model.save(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fineTune('../datasets/flowers17/images', 'flowers17.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
